# -*- coding: utf-8 -*-
"""plasmidgpt-exp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kXrzGtsDwaGNi6lBIH20nY2JBQKvrOH_

# Plasmid GPT Experiments
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# ! pip install ViennaRNA

from google.colab import drive
drive.mount('/content/drive')

! ls drive/MyDrive/plasmid-llm-results-nov-25/

import pandas as pd
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import plotly.offline as pyo
import os

base_path = "drive/MyDrive/plasmid-llm-results-nov-25/"

rl_df = pd.read_csv(os.path.join(base_path, 'RL/plasmids_unified.csv'))
rl_df['model'] = 'RL'

base_df = pd.read_csv(os.path.join(base_path, "base_model/plasmids_unified.csv"))
base_df['model'] = 'base_model'

sft_df = pd.read_csv(os.path.join(base_path, "angus_sft/plasmids_unified.csv"))
sft_df['model'] = 'sft_model'

combined = pd.concat([rl_df, base_df, sft_df], ignore_index=True)

TOTAL_PROMPTS_PER_TYPE = 50

summary_rows = []
for model in ['RL', 'base_model', 'sft_model']:
    model_df = combined[combined['model'] == model]

    for prompt in ['ATG', 'GFP_cassette']:
        prompt_df = model_df[model_df['prompt_type'] == prompt]

        sequences_generated = len(prompt_df)
        passed = (prompt_df['qc_status'] == 'pass').sum()
        failed_qc = (prompt_df['qc_status'] == 'fail').sum()
        generation_failures = TOTAL_PROMPTS_PER_TYPE - sequences_generated

        pass_rate = (passed / TOTAL_PROMPTS_PER_TYPE * 100)
        generation_success_rate = (sequences_generated / TOTAL_PROMPTS_PER_TYPE * 100)

        avg_novelty = prompt_df['pct_novelty'].mean()

        summary_rows.append({
            'model': model,
            'prompt': prompt,
            'total_prompts': TOTAL_PROMPTS_PER_TYPE,
            'sequences_generated': sequences_generated,
            'generation_failures': generation_failures,
            'passed_qc': passed,
            'failed_qc': failed_qc,
            'generation_success_rate': round(generation_success_rate, 2),
            'pass_rate': round(pass_rate, 2),
            'avg_novelty': round(avg_novelty, 2)
        })

summary_df = pd.DataFrame(summary_rows)
summary_df

summary_df.to_csv('model_comparison_summary.csv', index=False)


passed_seqs = combined[combined['qc_status'] == 'pass']
novelty_by_model = passed_seqs.pivot_table(index='model', columns='call', values='plasmid_id', aggfunc='count').fillna(0)
novelty_by_model = novelty_by_model.reindex(columns=['Novel', 'Similar', 'Exists'], fill_value=0)

total_passed_by_model = passed_seqs.groupby('model').size()
novelty_pct_by_model = novelty_by_model.div(total_passed_by_model, axis=0) * 100
novelty_pct_by_model = novelty_pct_by_model.reindex(['RL', 'sft_model', 'base_model'])

novel_and_pass = combined[(combined['call'] == 'Novel') & (combined['qc_status'] == 'pass')].groupby('model').size().reset_index(name='novel_and_pass')
all_models = pd.DataFrame({'model': ['RL', 'sft_model', 'base_model']})
novel_and_pass = all_models.merge(novel_and_pass, on='model', how='left').fillna(0)

import matplotlib.pyplot as plt
import seaborn as sns

# Use a simple style; we'll turn grids off explicitly
sns.set_theme(style="ticks")

fig, axes = plt.subplots(2, 2, figsize=(12, 9))
(ax11, ax12), (ax21, ax22) = axes

model_order = ['RL', 'sft_model', 'base_model']

# 1) Pass Rate by Model and Prompt (out of 50 prompts each)
pass_rate_data = summary_df[
    summary_df['prompt'].isin(['ATG', 'GFP_cassette'])
]

sns.barplot(
    data=pass_rate_data,
    x='model',
    y='pass_rate',
    hue='prompt',
    order=model_order,
    ax=ax11
)
ax11.set_title('Pass Rate by Model and Prompt (out of 50 prompts each)')
ax11.set_xlabel('Model')
ax11.set_ylabel('Pass Rate (%)')
ax11.grid(False)

# Add labels (skip zeros)
for p in ax11.patches:
    height = p.get_height()
    if height > 0:
        ax11.annotate(
            f'{height:.1f}%',
            (p.get_x() + p.get_width() / 2., height),
            ha='center', va='bottom', fontsize=9
        )

# 2) Novelty Classification by Model (% of sequences that passed QC)
plot_cols = [c for c in ['Novel', 'Similar', 'Exists']
             if c in novelty_pct_by_model.columns]

novelty_long = (
    novelty_pct_by_model[plot_cols]
    .reset_index()
    .rename(columns={'index': 'model'})
    .melt(id_vars='model', var_name='call_type', value_name='pct')
)

sns.barplot(
    data=novelty_long,
    x='model',
    y='pct',
    hue='call_type',
    order=model_order,
    ax=ax12
)
ax12.set_title('Novelty Classification by Model (% of sequences that passed QC)')
ax12.set_xlabel('Model')
ax12.set_ylabel('Percent')
ax12.grid(False)

for p in ax12.patches:
    height = p.get_height()
    if height > 0:
        ax12.annotate(
            f'{height:.1f}%',
            (p.get_x() + p.get_width() / 2., height),
            ha='center', va='bottom', fontsize=9
        )

# 3) Overall Pass Rate by Model (out of 100 total)
overall_by_model = (
    summary_df
    .groupby('model', as_index=False)['pass_rate']
    .mean()
    .set_index('model')
    .reindex(model_order)
    .reset_index()
)

sns.barplot(
    data=overall_by_model,
    x='model',
    y='pass_rate',
    order=model_order,
    ax=ax21
)
ax21.set_title('Overall Pass Rate by Model (out of 100 total)')
ax21.set_xlabel('Model')
ax21.set_ylabel('Pass Rate (%)')
ax21.grid(False)

for p in ax21.patches:
    height = p.get_height()
    if height > 0:
        ax21.annotate(
            f'{height:.1f}%',
            (p.get_x() + p.get_width() / 2., height),
            ha='center', va='bottom', fontsize=9
        )

# 4) Novel AND Passed QC by Model (out of 100 total)
sns.barplot(
    data=novel_and_pass,
    x='model',
    y='novel_and_pass',
    order=[m for m in model_order if m in novel_and_pass['model'].unique()],
    ax=ax22
)
ax22.set_title('Novel AND Passed QC by Model (out of 100 total)')
ax22.set_xlabel('Model')
ax22.set_ylabel('Count (Novel + Pass)')
ax22.grid(False)

for p in ax22.patches:
    height = p.get_height()
    if height > 0:
        ax22.annotate(
            f'{height:.0f}',
            (p.get_x() + p.get_width() / 2., height),
            ha='center', va='bottom', fontsize=9
        )

plt.tight_layout()
plt.show()

from scipy.stats import chi2_contingency, mannwhitneyu, kruskal, friedmanchisquare
from statsmodels.stats.multitest import multipletests

# 1. QC Pass Rate Comparison (Chi-square)
contingency = pd.crosstab(combined['model'], combined['qc_status'])
chi2, p_val, dof, expected = chi2_contingency(contingency)
print(f"Chi-square test p={p_val:.4e}")

# 2. Pairwise comparisons (with Bonferroni correction)
models_ = ['RL', 'sft_model', 'base_model']
p_values = []
for i, m1 in enumerate(models_):
    for m2 in models_[i+1:]:
        pass_m1 = (combined[combined['model']==m1]['qc_status']=='pass').values
        pass_m2 = (combined[combined['model']==m2]['qc_status']=='pass').values
        _, p = mannwhitneyu([1]*sum(pass_m1) + [0]*sum(~pass_m1),
                           [1]*sum(pass_m2) + [0]*sum(~pass_m2))
        p_values.append(p)
        print(f"{m1} vs {m2}: p={p:.4e}")

from statsmodels.stats.power import zt_ind_solve_power
# Check if n=50 was sufficient
effect_size = (0.81 - 0.06) / np.sqrt(0.81*0.19 + 0.06*0.94)  # Cohen's h
power = zt_ind_solve_power(effect_size, nobs1=50, alpha=0.05, alternative='two-sided')
print(f"Statistical power: {power:.2f}")  # Should be >0.8

"""# Part 2

These are more independant analysis experiments run after the initial QC.

## 0: Set up
"""

!pip install -q transformers torch seaborn matplotlib biopython scipy

import os
import math
import time
import json
from collections import Counter, defaultdict

import torch
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from Bio.Seq import Seq
from scipy.spatial.distance import jensenshannon
from transformers import AutoTokenizer, AutoModelForCausalLM
from tqdm.notebook import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# === Model IDs ===
BASE_ID = "UCL-CSSB/PlasmidGPT"
RL_ID   = "UCL-CSSB/PlasmidGPT-GRPO"
SFT_ID  = "UCL-CSSB/PlasmidGPT-SFT"

# === Load tokenizer and models ===
print("Loading tokenizer & models...")
tokenizer = AutoTokenizer.from_pretrained(RL_ID, trust_remote_code=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

def load_model(model_id):
    if model_id is None:
        return None
    m = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)
    return m.to(device).eval()

base_model = load_model(BASE_ID)
rl_model   = load_model(RL_ID)
sft_model  = load_model(SFT_ID)


models = {
    "Base": base_model,
    "SFT": sft_model,
    "RL": rl_model,
}
models = {k: v for k, v in models.items() if v is not None}

#check they are all on GPU
for name, model in models.items():
    print(name, model.device)

print("Loaded models:", list(models.keys()))

DNA_CHARS = set("ATGC")

def clean_dna(text: str) -> str:
    return "".join([c for c in text.upper() if c in DNA_CHARS])

def longest_orf_aa(seq_str: str) -> int:
    """Return length (aa) of longest ORF in any frame."""
    seq = Seq(seq_str)
    longest = 0
    for frame in range(3):
        try:
            prots = seq[frame:].translate().split("*")
            candidates = [len(p) for p in prots]
            if candidates:
                longest = max(longest, max(candidates))
        except Exception:
            pass
    return int(longest)

def count_orfs_above(seq_str: str, min_aa: int = 100) -> int:
    seq = Seq(seq_str)
    count = 0
    for frame in range(3):
        try:
            prots = seq[frame:].translate().split("*")
            count += sum(1 for p in prots if len(p) >= min_aa)
        except Exception:
            pass
    return int(count)

def gc_content(seq: str) -> float:
    seq = seq.upper()
    if not seq:
        return 0.0
    gc = seq.count("G") + seq.count("C")
    return gc / len(seq)

def kmer_distribution(seq: str, k: int = 3):
    seq = seq.upper()
    kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]
    counts = Counter(kmers)
    total = sum(counts.values())
    if total == 0:
        return {}
    return {kmer: c / total for kmer, c in counts.items()}

def js_divergence_kmers(seq_a: str, seq_b: str, k: int = 3) -> float:
    dist_a = kmer_distribution(seq_a, k=k)
    dist_b = kmer_distribution(seq_b, k=k)
    all_kmers = sorted(set(dist_a.keys()) | set(dist_b.keys()))
    if not all_kmers:
        return 0.0
    p = np.array([dist_a.get(km, 0.0) for km in all_kmers])
    q = np.array([dist_b.get(km, 0.0) for km in all_kmers])
    return float(jensenshannon(p, q, base=2.0))  # JS distance

REAL_DIR = "drive/MyDrive/annotations"

# List of real plasmids you want to compare against
real_fasta_files = [
    "pUC19.fasta",
    "pBR322.fasta",
    "pET-28a.fasta",
    "pACYC184.fasta",
    "pBAD24.fasta",
    "pEGFP.fasta",
    "pcDNA3.fasta",
    "pBluescript2.fasta",
    "px330.fasta",
]

def read_fasta(path):
    with open(path, "r") as f:
        lines = [l.strip() for l in f if not l.startswith(">")]
    return clean_dna("".join(lines))

real_plasmids = []
for fname in real_fasta_files:
    path = os.path.join(REAL_DIR, fname)
    if not os.path.exists(path):
        print("WARNING: missing", path)
        continue
    seq = read_fasta(path)
    real_plasmids.append({"Name": fname, "Sequence": seq})

len(real_plasmids), [p["Name"] for p in real_plasmids]

"""**Generating 500 samples from each of the 3 models with both the strong (GFP cassette) prompt and weat ATG prompt**"""

PROMPTS = {
    "Weak_ATG": "ATG",
    "GFP_cassette": open(os.path.join(REAL_DIR, "GFP_cassette.fasta")).read().splitlines()[-1][:150]
}

NUM_SAMPLES = 512
MAX_NEW_TOKENS=256

from tqdm.auto import tqdm
import math

def generate_sequences(
    model,
    model_name: str,
    prompt: str,
    num_samples: int = 100,
    max_new_tokens: int = 512,
    batch_size: int = 8,
):
    # Make sure model is on GPU
    model.to(device)
    model.eval()

    # Tokenize once
    inputs = tokenizer(prompt, return_tensors="pt")
    input_ids = inputs.input_ids.to(device)

    # Safety clamp
    if (input_ids >= model.config.vocab_size).any():
        input_ids[input_ids >= model.config.vocab_size] = 0

    all_out = []
    n_batches = math.ceil(num_samples / batch_size)

    for _ in tqdm(range(n_batches), desc=f"{model_name} / {prompt[:10]}"):
        cur_bs = min(batch_size, num_samples - len(all_out))
        # Repeat the prompt for the current batch
        input_batch = input_ids.repeat(cur_bs, 1)

        with torch.no_grad():
            # Optional: autocast for extra speed if you loaded model in fp16
            with torch.cuda.amp.autocast(enabled=True):
                gen = model.generate(
                    input_batch,
                    max_new_tokens=max_new_tokens,
                    do_sample=True,
                    top_p=0.92,
                    temperature=0.9,
                    pad_token_id=tokenizer.pad_token_id,
                )

        # Decode each sequence in the batch
        for row in gen:
            text = tokenizer.decode(row, skip_special_tokens=True)
            dna = clean_dna(text)
            all_out.append(dna)

    # Trim in case we overshot
    return all_out[:num_samples]


need_to_generate = True

try:
    if generated:
        need_to_generate = False
except NameError:
    need_to_generate = True

if need_to_generate:

    generated = []  # list of dicts: {Model, Prompt, Sequence}
    for model_name, model in models.items():
        print("Generating for", model_name)
        for prompt_name, prompt_seq in PROMPTS.items():
            seqs = generate_sequences(
                model,
                model_name,
                prompt_seq,
                num_samples=NUM_SAMPLES,
                max_new_tokens=MAX_NEW_TOKENS,
                batch_size=16,  # tweak 4/8/16 depending on VRAM
            )
            for s in seqs:
                generated.append({
                    "Model": model_name,
                    "Prompt": prompt_name,
                    "Sequence": s,
                })

gen_df = pd.DataFrame(generated)
gen_df.head()

"""## Experiment #1

With the large number of samples we just generated we want to calculate a bunch of summary statistics then compare some of the summary stats to real plasmids and see which models reflect each of the attributes best

**TODO:** add more real plasmids to compare (we are using like 9 right now

**Metrics**
* Length
* GC Content
* Longest ORF
* Number of ORF >= 100 AA
* JS divergence of k-mer distributions
    * “How different is the model’s k-mer frequency distribution from real plasmids?”
    * For a given sequence:

        1. Count all 3-mers

        2. Normalize to a frequency distribution

        3. Compute JS divergence between the model’s distribution, and the empirical distribution from real plasmids
* structual stability from viennaRNA
"""

from Bio.Seq import Seq
import pandas as pd
import numpy as np
import RNA



def get_circular_mfe(seq_str: str) -> tuple[float, float]:
    """
    Calculates Minimum Free Energy for circular DNA.
    Returns: (Total MFE, MFE Density per nt)
    """

    # Configure model details for circular sequence
    md = RNA.md()
    md.circ = 1  # 1 = Circular
    # Note: Default parameters are RNA. For strict DNA accuracy,
    # one would load DNA parameters, but standard folding is a
    # widely accepted proxy for structural complexity in this context.

    # Create fold compound
    fc = RNA.fold_compound(seq_str, md)

    # Calculate MFE (structure string, energy float)
    (structure, mfe) = fc.mfe()

    # Density: kcal/mol per nucleotide
    density = mfe / len(seq_str) if len(seq_str) > 0 else 0.0

    return mfe, density

def longest_orf_any_frame(seq_str: str) -> int:
    """Longest run between stop codons in any of the 3 frames (aa)."""
    seq = Seq(seq_str)
    longest = 0
    for frame in range(3):
        try:
            prots = seq[frame:].translate().split("*")
            if prots:
                longest = max(longest, max(len(p) for p in prots))
        except Exception:
            pass
    return int(longest)

def longest_orf_atg_single_strand(seq_str: str, min_aa: int = 0) -> int:
    """
    Longest ORF that starts with ATG and ends at a stop (TAA/TAG/TGA)
    on a single strand, across all 3 frames.
    """
    s = seq_str.upper()
    longest = 0
    stop_codons = {"TAA", "TAG", "TGA"}
    for frame in range(3):
        i = frame
        while i + 3 <= len(s):
            codon = s[i:i+3]
            if codon == "ATG":
                j = i + 3
                while j + 3 <= len(s):
                    stop = s[j:j+3]
                    if stop in stop_codons:
                        length_aa = (j - i) // 3
                        if length_aa >= min_aa:
                            longest = max(longest, length_aa)
                        break
                    j += 3
                i = j
            else:
                i += 3
    return int(longest)

def longest_orf_atg_both_strands(seq_str: str, min_aa: int = 0) -> tuple[int, int, int]:
    """
    Longest ATG-started ORF on:
      - forward strand,
      - reverse complement,
      - and the max of both.
    """
    fwd = longest_orf_atg_single_strand(seq_str, min_aa=min_aa)
    rev_seq = str(Seq(seq_str).reverse_complement())
    rev = longest_orf_atg_single_strand(rev_seq, min_aa=min_aa)
    both = max(fwd, rev)
    return fwd, rev, both

def compute_metrics_for_seq(seq, reference_concat_seq):
    # Ensure seq is a string for ViennaRNA
    seq_str = str(seq)
    L = len(seq_str)

    gc = gc_content(seq_str) # Assuming this function exists elsewhere in your code

    longest_any = longest_orf_any_frame(seq_str)
    n_orfs = count_orfs_above(seq_str, min_aa=100) # Assuming exists elsewhere


    js3 = js_divergence_kmers(seq_str, reference_concat_seq, k=3) # Assuming exists

    atg_fwd, atg_rev, atg_both = longest_orf_atg_both_strands(seq_str, min_aa=0)

    mfe_total, mfe_density = get_circular_mfe(seq_str)

    return {
        "Length": L,
        "GC": gc,
        "Longest_ORF_AA_any": longest_any,
        "Num_ORFs_>=100AA": n_orfs,
        "JS_3mer_vs_real": js3,
        "Longest_ORF_ATG_fwd": atg_fwd,
        "Longest_ORF_ATG_rev": atg_rev,
        "Longest_ORF_ATG_both": atg_both,
        "MFE_Total": mfe_total,
        "MFE_Density": mfe_density
    }


# Concatenate all real sequences to build a "background" 3-mer distribution
# real_plasmids list is assumed to exist from context
real_concat = "".join([p["Sequence"] for p in real_plasmids])

metrics_df = pd.read_csv("drive/MyDrive/plasmid-llm-results-nov-25/metrics/metrics.csv")
real_df = pd.read_csv("drive/MyDrive/plasmid-llm-results-nov-25/metrics/real.csv")

! lscpu

# Commented out IPython magic to ensure Python compatibility.
# 
# 
# %%skip
# #this takes forever try to load from save if possible
# 
# need_real = True
# try:
#     if real_rows:
#         need_real = False
# except NameError:
#     print("Generating real df")
#     need_real = True
# 
# if need_real:
# 
#     # Metrics for real plasmids
#     real_rows = []
#     for p in tqdm(real_plasmids):
#         m = compute_metrics_for_seq(p["Sequence"], real_concat)
#         m.update({"Model": "Real", "Prompt": "Real", "Name": p.get("Name", "Unknown")})
#         real_rows.append(m)
# 
#     real_df = pd.DataFrame(real_rows)
# 
# need_metrics = True
# try:
#     if metric_rows:
#         need_metrics = False
# except NameError:
#     print("generatig metrics")
#     need_metrics = True
# 
# if need_metrics:
#     # Metrics for generated sequences
#     metric_rows = []
#     for idx, row in tqdm(gen_df.iterrows()):
#         m = compute_metrics_for_seq(row["Sequence"], real_concat)
#         m.update({"Model": row["Model"], "Prompt": row["Prompt"]})
#         metric_rows.append(m)
# 
#     metrics_df = pd.concat([real_df, pd.DataFrame(metric_rows)], ignore_index=True)
# 
# # Display head to check new columns
# print(metrics_df[["Model", "Length", "Longest_ORF_ATG_both", "MFE_Density"]].head())

import multiprocessing
import re
from Bio.Seq import Seq
import pandas as pd
import numpy as np
from tqdm import tqdm
# Try/Except for ViennaRNA in case this runs where it's not installed
try:
    import RNA
except ImportError:
    pass

# --- 1. Optimized Core Functions ---

def get_circular_mfe(seq_str: str) -> tuple[float, float]:
    """Calculates Minimum Free Energy for circular DNA."""
    if not seq_str: return 0.0, 0.0

    # Create model locally (Process-safe)
    md = RNA.md()
    md.circ = 1
    fc = RNA.fold_compound(str(seq_str), md)
    (structure, mfe) = fc.mfe()
    return mfe, mfe / len(seq_str)

def fast_longest_orf_atg(seq_str: str, min_aa: int = 0) -> int:
    """
    Optimized ORF finder using Biopython translate + Regex.
    ~100x faster than manual loops.
    """
    seq = Seq(seq_str)
    max_len = 0

    # Check all 3 frames
    for frame in range(3):
        # Translate full sequence (fast C-implementation)
        # to_stop=False allows us to see multiple ORFs in one frame
        prot_seq = str(seq[frame:].translate())

        # Regex Explanation:
        # M      : Starts with Methionine (ATG)
        # [^*]* : Zero or more non-stop characters
        # \* : Ends with a Stop codon (required by your logic)
        matches = re.findall(r'M[^*]*\*', prot_seq)

        if matches:
            # len(m) - 1 excludes the stop codon from the AA count
            longest_in_frame = max(len(m) - 1 for m in matches)
            if longest_in_frame >= min_aa:
                max_len = max(max_len, longest_in_frame)

    return int(max_len)

def get_orfs_both_strands_fast(seq_str: str) -> tuple[int, int, int]:
    """Wraps the fast finder for both strands."""
    fwd = fast_longest_orf_atg(seq_str)
    rev_seq = str(Seq(seq_str).reverse_complement())
    rev = fast_longest_orf_atg(rev_seq)
    return fwd, rev, max(fwd, rev)

# --- 2. Worker Function for Parallelization ---

def process_single_plasmid(args):
    """
    Worker function that processes one plasmid.
    Args:
        data_tuple: (sequence_string, reference_string, model_name, prompt_name)
    """
    seq_str, ref_concat, model_tag, prompt_tag, name_tag = args

    # 1. ORF Metrics (Fast)
    atg_fwd, atg_rev, atg_both = get_orfs_both_strands_fast(seq_str)

    # 2. Structural Metrics (Heavy)
    mfe_total, mfe_density = get_circular_mfe(seq_str)

    # 3. Other metrics (Assuming these functions are defined in your scope)
    # If these functions (gc_content, etc) are simple, they are fine.
    # If they are heavy, optimize them too.
    gc = (seq_str.count('G') + seq_str.count('C')) / len(seq_str) if seq_str else 0

    # Re-implement simple placeholders if missing from context
    try:
        js3 = js_divergence_kmers(seq_str, ref_concat, k=3)
        longest_any = longest_orf_any_frame(seq_str)
        n_orfs = count_orfs_above(seq_str, min_aa=100)
    except NameError:
        # Fallback if functions aren't passed to worker
        js3, longest_any, n_orfs = 0, 0, 0

    return {
        "Model": model_tag,
        "Prompt": prompt_tag,
        "Name": name_tag,
        "Length": len(seq_str),
        "GC": gc,
        "Longest_ORF_AA_any": longest_any,
        "Num_ORFs_>=100AA": n_orfs,
        "JS_3mer_vs_real": js3,
        "Longest_ORF_ATG_fwd": atg_fwd,
        "Longest_ORF_ATG_rev": atg_rev,
        "Longest_ORF_ATG_both": atg_both,
        "MFE_Total": mfe_total,
        "MFE_Density": mfe_density
    }

# --- 3. Parallel Execution Block ---

if __name__ == "__main__":
    # Prepare arguments for all real plasmids
    # Format: (Seq, Ref_Concat, Model, Prompt, Name)

    # 1. Setup Data
    real_concat = "".join([p["Sequence"] for p in real_plasmids])

    tasks = []

    # Add Real Plasmids
    for p in real_plasmids:
        tasks.append((p["Sequence"], real_concat, "Real", "Real", p.get("Name", "Unknown")))

    # Add Generated Plasmids
    for idx, row in gen_df.iterrows():
        tasks.append((row["Sequence"], real_concat, row["Model"], row["Prompt"], f"Gen_{idx}"))

    print(f"Processing {len(tasks)} sequences on {multiprocessing.cpu_count()} cores...")

    # 2. Run Parallel Pool
    results = []
    with multiprocessing.Pool(processes=multiprocessing.cpu_count()) as pool:
        # imap_unordered is often faster for progress bars
        for res in tqdm(pool.imap(process_single_plasmid, tasks), total=len(tasks)):
            results.append(res)

    # 3. Create DataFrame
    metrics_df = pd.DataFrame(results)

    print("\nDone!")
    print(metrics_df[["Model", "Length", "Longest_ORF_ATG_both", "MFE_Density"]].head())

! drive/MyDrive/plasmid-llm-results-nov-25/metrics
metrics_df.to_csv("drive/MyDrive/plasmid-llm-results-nov-25/metrics/metrics.csv")
real_df.to_csv("drive/MyDrive/plasmid-llm-results-nov-25/metrics/real.csv")

import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(style="whitegrid", context="paper", font_scale=1.2)

# Order models: Real first, RL last
model_order = ["Real", "Base", "SFT", "RL"]
metrics_df["Model"] = pd.Categorical(metrics_df["Model"], categories=model_order, ordered=True)

# ---- 2.1 Length (log10) ----
metrics_df["log10_Length"] = np.log10(metrics_df["Length"].clip(lower=1))

plt.figure(figsize=(8, 4))
sns.boxplot(
    data=metrics_df,
    x="Model",
    y="log10_Length",
    hue="Prompt",
    dodge=True,
    showfliers=False,
)
sns.stripplot(
    data=metrics_df,
    x="Model",
    y="log10_Length",
    hue="Prompt",
    dodge=True,
    alpha=0.4,
    marker="o",
    linewidth=0,
)
plt.ylabel("log10(Length in bp)")
plt.title("Sequence length distributions")
plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# ---- 2.2 GC content ----
plt.figure(figsize=(8, 4))
sns.boxplot(
    data=metrics_df,
    x="Model",
    y="GC",
    hue="Prompt",
    dodge=True,
    showfliers=False,
)
sns.stripplot(
    data=metrics_df,
    x="Model",
    y="GC",
    hue="Prompt",
    dodge=True,
    alpha=0.4,
    marker="o",
    linewidth=0,
)

real_gc_mean = real_df["GC"].mean()
plt.axhline(real_gc_mean, linestyle="--", linewidth=1, color="black", label="Real mean")
plt.ylabel("GC fraction")
plt.title("GC content vs real plasmids")
plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# ---- 2.3 ORF metrics (any-frame + ATG/both strands), log-scale ----
for col, title in [
    ("Longest_ORF_AA_any", "Longest ORF (any frame, no ATG requirement)"),
    ("Longest_ORF_ATG_fwd", "Longest ATG-start ORF (forward strand)"),
    ("Longest_ORF_ATG_rev", "Longest ATG-start ORF (reverse complement)"),
    ("Longest_ORF_ATG_both", "Longest ATG-start ORF (either strand)"),
]:
    log_col = f"log10_{col}"
    metrics_df[log_col] = np.log10(metrics_df[col].clip(lower=1))

    plt.figure(figsize=(8, 4))
    sns.boxplot(
        data=metrics_df,
        x="Model",
        y=log_col,
        hue="Prompt",
        dodge=True,
        showfliers=False,
    )
    sns.stripplot(
        data=metrics_df,
        x="Model",
        y=log_col,
        hue="Prompt",
        dodge=True,
        alpha=0.4,
        marker="o",
        linewidth=0,
    )
    plt.ylabel("log10(length in aa)")
    plt.title(title)
    plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
    plt.tight_layout()
    plt.show()

# ---- 2.4 Number of ORFs ≥ 100 aa ----
metrics_df["log10_Num_ORFs"] = np.log10(metrics_df["Num_ORFs_>=100AA"].clip(lower=1))

plt.figure(figsize=(8, 4))
sns.boxplot(
    data=metrics_df,
    x="Model",
    y="log10_Num_ORFs",
    hue="Prompt",
    dodge=True,
    showfliers=False,
)
sns.stripplot(
    data=metrics_df,
    x="Model",
    y="log10_Num_ORFs",
    hue="Prompt",
    dodge=True,
    alpha=0.4,
    marker="o",
    linewidth=0,
)
plt.ylabel("log10(# ORFs ≥ 100 aa)")
plt.title("Gene-like content per plasmid")
plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

# ---- 2.5 JS divergence (3-mer) ----
plt.figure(figsize=(8, 4))
sns.boxplot(
    data=metrics_df,
    x="Model",
    y="JS_3mer_vs_real",
    hue="Prompt",
    dodge=True,
    showfliers=False,
)
sns.stripplot(
    data=metrics_df,
    x="Model",
    y="JS_3mer_vs_real",
    hue="Prompt",
    dodge=True,
    alpha=0.4,
    marker="o",
    linewidth=0,
)
plt.ylabel("JS divergence (3-mer vs real)")
plt.title("Codon-level similarity to real plasmids")
plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

plt.figure(figsize=(8, 4))
sns.boxplot(
    data=metrics_df,
    x="Model",
    y="MFE_Density",
    hue="Prompt",
    dodge=True,
    showfliers=False,
)
sns.stripplot(
    data=metrics_df,
    x="Model",
    y="MFE_Density",
    hue="Prompt",
    dodge=True,
    alpha=0.4,
    marker="o",
    linewidth=0,
)

# Add a reference line for the mean of Real plasmids
# (assumes 'metrics_df' contains the 'Real' data from the concat step)
real_mfe_mean = metrics_df[metrics_df["Model"] == "Real"]["MFE_Density"].mean()
plt.axhline(real_mfe_mean, linestyle="--", linewidth=1, color="black", label="Real mean")

plt.ylabel("MFE Density (kcal/mol/nt)")
plt.title("Thermodynamic Stability (MFE Density)")
plt.legend(title="Prompt", bbox_to_anchor=(1.05, 1), loc="upper left")
plt.tight_layout()
plt.show()

from scipy.stats import wasserstein_distance

# 1. Define the ground truth
real_vals = metrics_df[metrics_df.Model == "Real"]["Length"].dropna()

# 2. Calculate Distance for Base (The "Before")
base_vals = metrics_df[metrics_df.Model == "Base"]["Length"].dropna()
base_dist = wasserstein_distance(base_vals, real_vals)

# 3. Calculate Distance for RL (The "After")
rl_vals = metrics_df[metrics_df.Model == "RL"]["Length"].dropna()
rl_dist = wasserstein_distance(rl_vals, real_vals)

print(f"Distance to Reality (Lower is Better):")
print(f"Base Model: {base_dist:.4f}")
print(f"RL Model:   {rl_dist:.4f}")
print(f"Improvement: {(1 - rl_dist/base_dist)*100:.1f}%")

"""## Experiment #2

**Held-out continuation**



> “Given the first 400 bases of a real plasmid, how well can the model predict the next 100 bases?”

This is an auto complete model. We are checking how good each of the models are by running auto complete and checking how well it does on the last part.
"""

def sequence_logprob(model, prefix: str, target: str) -> float:
    """
    Average log-prob per token of `target` given `prefix`.

    We tokenize prefix+target once, then slice out the log-probs for the target region.
    """
    full = prefix + target
    enc_full = tokenizer(full, return_tensors="pt")
    input_ids = enc_full.input_ids.to(device)
    attn_mask = enc_full.attention_mask.to(device)

    # Safety clamp
    if (input_ids >= model.config.vocab_size).any():
        input_ids[input_ids >= model.config.vocab_size] = 0

    with torch.no_grad():
        logits = model(input_ids, attention_mask=attn_mask).logits  # [1, T, V]
    log_probs = torch.nn.functional.log_softmax(logits, dim=-1)

    # Shift for next-token prediction
    target_ids = input_ids[:, 1:]
    log_probs = log_probs[:, :-1, :]  # now aligned with target_ids

    # Figure out where the target starts in token space
    prefix_ids = tokenizer(prefix, return_tensors="pt").input_ids
    prefix_len = prefix_ids.shape[1]
    full_len = input_ids.shape[1]

    # Positions in token space corresponding to the target substring
    target_slice = slice(prefix_len - 1, full_len - 1)

    relevant_target_ids = target_ids[:, target_slice]
    relevant_log_probs = log_probs[:, target_slice, :]

    gathered = relevant_log_probs.gather(2, relevant_target_ids.unsqueeze(-1)).squeeze(-1)
    return float(gathered.mean().cpu())

WINDOW_PREFIX = 400
WINDOW_TARGET = 100
CUT_STRIDE = 300  # how far apart cut points are along the plasmid

completion_results = []

for plasmid in real_plasmids:
    seq = plasmid["Sequence"]
    name = plasmid["Name"]
    if len(seq) < WINDOW_PREFIX + WINDOW_TARGET + 10:
        continue

    for start in range(0, len(seq) - (WINDOW_PREFIX + WINDOW_TARGET), CUT_STRIDE):
        prefix = seq[start : start + WINDOW_PREFIX]
        target = seq[start + WINDOW_PREFIX : start + WINDOW_PREFIX + WINDOW_TARGET]

        for model_name, model in models.items():
            lp = sequence_logprob(model, prefix, target)
            completion_results.append({
                "Plasmid": name,
                "Model": model_name,
                "PrefixStart": start,
                "AvgLogProb": lp,
            })

completion_df = pd.DataFrame(completion_results)
completion_df.head()

# Ensure model ordering matches main metrics
completion_df["Model"] = pd.Categorical(completion_df["Model"], categories=model_order, ordered=True)

plt.figure(figsize=(6, 4))
sns.boxplot(
    data=completion_df,
    x="Model",
    y="AvgLogProb",
    showfliers=False,
)
sns.stripplot(
    data=completion_df,
    x="Model",
    y="AvgLogProb",
    color="black",
    alpha=0.4,
    jitter=True,
)
plt.title("Held-out completion:\nlog-prob of next 100 bp given 400 bp prefix")
plt.ylabel("Avg log-prob per token (higher is better)")
plt.tight_layout()
plt.show()

print(completion_df.groupby("Model")["AvgLogProb"].mean())

from scipy.stats import ttest_rel

# 1. Pivot to ensure strict alignment of pairs
pivoted = completion_df.pivot_table(
    index=["Plasmid", "PrefixStart"],
    columns="Model",
    values="AvgLogProb"
).dropna()

# 2. Extract aligned arrays
base_vals = pivoted["Base"]
rl_vals = pivoted["RL"]

# 3. Run Test
t_stat, p = ttest_rel(base_vals, rl_vals)

print(f"Mean Base: {base_vals.mean():.4f}")
print(f"Mean RL:   {rl_vals.mean():.4f}")
print(f"Paired t-test: t={t_stat:.3f}, p={p:.4e}")

"""## Experiment #3 - Promoter → CDS surprisal gap

This experiment tries to answer a simple question:

> Does the RL-trained model think real promoter→CDS regions “look right” more than the Base model does?

Real plasmids have a pretty standard pattern:
a promoter, then shortly after, a CDS (a gene).
If a model has learned real plasmid structure, those regions should feel predictable to it.

**How we measure it**

We extract a small window of real DNA around each promoter→CDS boundary.
For each window, we compute:

the average log-probability per token assigned by the Base model

the same value assigned by the RL model

Then we take the difference:

$gap = logprob_{RL} - logprob_{base}$

- Positive gap → RL finds that region more predictable (less surprising) than Base

- Negative gap → RL finds it more surprising than Base

**How to interpret the plot**

Each dot is one promoter→CDS window from a real plasmid.

If most dots are above zero, it means RL generally understands those promoter→gene transitions better.

If some dots fall below zero, that just means there are a few cases where RL isn’t better (which is fine — we have very few datapoints).


"""

# List of annotation CSVs (one per plasmid)
annot_files = [
    "pUC19_pLann.csv",
    "pBR322_pLann.csv",
    "pET-28a_pLann.csv",
    "pACYC184_pLann.csv",
    "pBAD24_pLann.csv",
    "pEGFP_pLann.csv",
]

annots = {}  # name -> dataframe
for fname in annot_files:
    path = os.path.join(REAL_DIR, fname)
    if os.path.exists(path):
        df = pd.read_csv(path)
        annots[fname] = df
    else:
        print("Missing annotation CSV:", path)

list(annots.keys())

# quick helper to get plasmid sequence by basename
real_seq_by_name = {os.path.splitext(p["Name"])[0]: p["Sequence"] for p in real_plasmids}
real_seq_by_name

def logprob_trace(model, seq: str):
    """Return per-token log-prob trace for the literal DNA string `seq`."""
    enc = tokenizer(seq, return_tensors="pt")
    input_ids = enc.input_ids.to(device)
    attn_mask = enc.attention_mask.to(device)

    # clamp
    if (input_ids >= model.config.vocab_size).any():
        input_ids[input_ids >= model.config.vocab_size] = 0

    with torch.no_grad():
        logits = model(input_ids, attention_mask=attn_mask).logits  # [1, T, V]
    lp = torch.nn.functional.log_softmax(logits, dim=-1)  # [1, T, V]

    target_ids = input_ids[:, 1:]
    lp = lp[:, :-1, :]
    gathered = lp.gather(2, target_ids.unsqueeze(-1)).squeeze(-1).cpu().numpy()[0]
    # gathered[i] is logprob of token i+1 given prefix up to i
    return gathered  # length T-1

WINDOW_BP = 100  # window around CDS start
surprisal_rows = []

for annot_name, df in annots.items():
    base_name = annot_name.split("_pLann.csv")[0]  # e.g., "pUC19"
    if base_name not in real_seq_by_name:
        print("No sequence for", base_name)
        continue
    seq = real_seq_by_name[base_name]

    # Normalize column names: lowercase + replace spaces with underscores
    norm_cols = {c.lower().replace(" ", "_"): c for c in df.columns}

    # We expect: "type", "start_location", "end_location"
    required = ("type", "start_location", "end_location")
    if not all(k in norm_cols for k in required):
        print("Unexpected columns in", annot_name, df.columns)
        continue

    type_col  = norm_cols["type"]              # -> "Type"
    start_col = norm_cols["start_location"]    # -> "start location"
    end_col   = norm_cols["end_location"]      # -> "end location"
    strand_col = norm_cols.get("strand", None) # -> "strand" if present

    # Grab promoters & CDS by Type column
    # (your Type values should look like "promoter", "CDS", etc.)
    type_series = df[type_col].astype(str).str.lower()
    promoters = df[type_series == "promoter"]
    cdss      = df[type_series == "cds"]

    if promoters.empty or cdss.empty:
        continue

    # For each promoter, find nearest downstream CDS on same strand
    for _, p in promoters.iterrows():
        # Convert to 0-based indices for Python slicing
        p_start = int(p[start_col]) - 1
        p_end   = int(p[end_col])   - 1
        p_strand = "+"
        if strand_col is not None and not pd.isna(p[strand_col]):
            p_strand = str(p[strand_col])

        # Filter CDS on same strand if strand info available
        if strand_col is not None:
            same_strand = cdss[cdss[strand_col].astype(str) == p_strand]
        else:
            same_strand = cdss

        if same_strand.empty:
            continue

        if p_strand == "+":
            cand = same_strand[same_strand[start_col] - 1 >= p_end]
        else:
            # For "-" strand reverse direction: CDS end <= promoter start
            cand = same_strand[same_strand[end_col] - 1 <= p_start]

        if cand.empty:
            continue

        c = cand.sort_values(start_col).iloc[0]
        cds_start = int(c[start_col]) - 1  # 0-based

        # Extract window around CDS start (in nucleotide space, 0-based)
        left = max(0, cds_start - WINDOW_BP)
        right = min(len(seq), cds_start + WINDOW_BP)
        window_seq = seq[left:right]
        if len(window_seq) < 50:
            continue

        # Compute logprob traces for Base and RL
        base_lp = logprob_trace(base_model, window_seq)
        rl_lp   = logprob_trace(rl_model, window_seq)

        # Align lengths
        L = min(len(base_lp), len(rl_lp))
        base_lp = base_lp[:L]
        rl_lp = rl_lp[:L]

        diff = rl_lp - base_lp  # positive => RL more confident
        mean_diff = float(diff.mean())

        surprisal_rows.append({
            "Plasmid": base_name,
            "PromoterStart_bp": p_start + 1,  # back to 1-based for reporting
            "CDSStart_bp": cds_start + 1,
            "WindowLenTokens": L,
            "MeanLogProbDiff": mean_diff,
        })

surprisal_df = pd.DataFrame(surprisal_rows)
surprisal_df.head(), len(surprisal_df)

plt.figure(figsize=(5, 4))
sns.stripplot(
    data=surprisal_df,
    x=[0] * len(surprisal_df),
    y="MeanLogProbDiff",
    jitter=0.2,
)
plt.axhline(0, color="black", linestyle="--", linewidth=1)
plt.xticks([], [])
plt.ylabel("Mean log-prob difference (RL - Base)")
plt.title("Surprisal gap at promoter→CDS windows")
plt.tight_layout()
plt.show()

print("N cassette windows:", len(surprisal_df))
print("Mean gap:", surprisal_df["MeanLogProbDiff"].mean())
print("Median gap:", surprisal_df["MeanLogProbDiff"].median())

